{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-31T17:20:04.493118300Z",
     "start_time": "2023-10-31T17:20:04.296967400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            town      flat_model  floor_area_sqm  lease_commence_date  \\\n0     ang mo kio  new generation            67.0                 1979   \n1  choa chu kang        improved           110.0                 2003   \n2       clementi  new generation            67.0                 1978   \n3    bukit batok         model a            74.0                 1984   \n4      sembawang         model a            93.0                 2002   \n\n   latitude   longitude            subzone  planning_area             region  \\\n0  1.366600  103.855579         chong boon     ang mo kio  north-east region   \n1  1.401158  103.751472            yew tee  choa chu kang        west region   \n2  1.316994  103.768507     clementi north       clementi        west region   \n3  1.347162  103.742808   bukit batok west    bukit batok        west region   \n4  1.450165  103.819307  sembawang central      sembawang       north region   \n\n   monthly_rent  ...  mall_nearest_distance    mall_nearest  \\\n0          2450  ...                 1036.0  Jubilee Square   \n1          2950  ...                  598.0  Yew Tee Square   \n2          1950  ...                  247.0    321 Clementi   \n3          1800  ...                  788.0       West Mall   \n4          2200  ...                  220.0       Sun Plaza   \n\n   mall_count_within_1km  mrt_planned_nearest_distance  mrt_planned_nearest  \\\n0                      0                         765.0           Ang Mo Kio   \n1                      2                         609.0              Yew Tee   \n2                      3                         424.0         Commonwealth   \n3                      1                         775.0          Bukit Batok   \n4                      1                         157.0            Sembawang   \n\n   mrt_planned_count_within_1km  flat_type_ordinal  \\\n0                             1                  2   \n1                             1                  4   \n2                             2                  2   \n3                             1                  2   \n4                             1                  3   \n\n  distance_to_centroid_marina_bay  lease_duration  school_score  \n0                        8.879467              44         336.6  \n1                       17.988626              20         159.2  \n2                       11.391051              45         330.5  \n3                       15.280442              39         275.9  \n4                       18.835740              21         319.5  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>town</th>\n      <th>flat_model</th>\n      <th>floor_area_sqm</th>\n      <th>lease_commence_date</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>subzone</th>\n      <th>planning_area</th>\n      <th>region</th>\n      <th>monthly_rent</th>\n      <th>...</th>\n      <th>mall_nearest_distance</th>\n      <th>mall_nearest</th>\n      <th>mall_count_within_1km</th>\n      <th>mrt_planned_nearest_distance</th>\n      <th>mrt_planned_nearest</th>\n      <th>mrt_planned_count_within_1km</th>\n      <th>flat_type_ordinal</th>\n      <th>distance_to_centroid_marina_bay</th>\n      <th>lease_duration</th>\n      <th>school_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ang mo kio</td>\n      <td>new generation</td>\n      <td>67.0</td>\n      <td>1979</td>\n      <td>1.366600</td>\n      <td>103.855579</td>\n      <td>chong boon</td>\n      <td>ang mo kio</td>\n      <td>north-east region</td>\n      <td>2450</td>\n      <td>...</td>\n      <td>1036.0</td>\n      <td>Jubilee Square</td>\n      <td>0</td>\n      <td>765.0</td>\n      <td>Ang Mo Kio</td>\n      <td>1</td>\n      <td>2</td>\n      <td>8.879467</td>\n      <td>44</td>\n      <td>336.6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>choa chu kang</td>\n      <td>improved</td>\n      <td>110.0</td>\n      <td>2003</td>\n      <td>1.401158</td>\n      <td>103.751472</td>\n      <td>yew tee</td>\n      <td>choa chu kang</td>\n      <td>west region</td>\n      <td>2950</td>\n      <td>...</td>\n      <td>598.0</td>\n      <td>Yew Tee Square</td>\n      <td>2</td>\n      <td>609.0</td>\n      <td>Yew Tee</td>\n      <td>1</td>\n      <td>4</td>\n      <td>17.988626</td>\n      <td>20</td>\n      <td>159.2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>clementi</td>\n      <td>new generation</td>\n      <td>67.0</td>\n      <td>1978</td>\n      <td>1.316994</td>\n      <td>103.768507</td>\n      <td>clementi north</td>\n      <td>clementi</td>\n      <td>west region</td>\n      <td>1950</td>\n      <td>...</td>\n      <td>247.0</td>\n      <td>321 Clementi</td>\n      <td>3</td>\n      <td>424.0</td>\n      <td>Commonwealth</td>\n      <td>2</td>\n      <td>2</td>\n      <td>11.391051</td>\n      <td>45</td>\n      <td>330.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bukit batok</td>\n      <td>model a</td>\n      <td>74.0</td>\n      <td>1984</td>\n      <td>1.347162</td>\n      <td>103.742808</td>\n      <td>bukit batok west</td>\n      <td>bukit batok</td>\n      <td>west region</td>\n      <td>1800</td>\n      <td>...</td>\n      <td>788.0</td>\n      <td>West Mall</td>\n      <td>1</td>\n      <td>775.0</td>\n      <td>Bukit Batok</td>\n      <td>1</td>\n      <td>2</td>\n      <td>15.280442</td>\n      <td>39</td>\n      <td>275.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sembawang</td>\n      <td>model a</td>\n      <td>93.0</td>\n      <td>2002</td>\n      <td>1.450165</td>\n      <td>103.819307</td>\n      <td>sembawang central</td>\n      <td>sembawang</td>\n      <td>north region</td>\n      <td>2200</td>\n      <td>...</td>\n      <td>220.0</td>\n      <td>Sun Plaza</td>\n      <td>1</td>\n      <td>157.0</td>\n      <td>Sembawang</td>\n      <td>1</td>\n      <td>3</td>\n      <td>18.835740</td>\n      <td>21</td>\n      <td>319.5</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 29 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training set\n",
    "train_set_path = './Datasets/final-data/train_set.csv'\n",
    "train_set = pd.read_csv(train_set_path)\n",
    "\n",
    "# Display the first few rows of the training set to understand its structure\n",
    "train_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "monthly_rent                       1.000000\ncoe_price                          0.523555\nrent_approval_year                 0.504592\nYear                               0.504592\nflat_type_ordinal                  0.325429\nfloor_area_sqm                     0.310135\nStock_Price                        0.298779\nlease_commence_date                0.229695\nmrt_planned_count_within_1km       0.086577\nmrt_count_within_1km               0.086457\nmall_count_within_1km              0.062640\nmrt_planned_nearest                0.039457\nmrt_nearest                        0.035967\nrent_approval_month                0.033383\nMonth                              0.033383\nsubzone                            0.007981\nplanning_area                      0.004862\nlongitude                         -0.000876\ntown                              -0.002161\nmall_nearest                      -0.024042\nschool_score                      -0.039169\nmrt_nearest_distance              -0.063785\nmrt_planned_nearest_distance      -0.067542\nmall_nearest_distance             -0.074378\ndistance_to_centroid_marina_bay   -0.074820\nregion                            -0.077193\nflat_model                        -0.102203\nlatitude                          -0.112890\nlease_duration                    -0.229695\nName: monthly_rent, dtype: float64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = train_set.corr()\n",
    "\n",
    "# # Plotting the heatmap of the correlation matrix\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "# plt.title('Correlation Heatmap')\n",
    "# plt.show()\n",
    "\n",
    "# Showing the correlation of features with the target variable 'monthly_rent'\n",
    "correlation_with_target = correlation_matrix['monthly_rent'].sort_values(ascending=False)\n",
    "correlation_with_target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T17:22:05.324041700Z",
     "start_time": "2023-10-31T17:22:05.230120200Z"
    }
   },
   "id": "81a4a1f4364f9e75"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(390.4274266323954, 269936.60460674297, 0.4700704576062311)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Preprocessing: Encoding categorical features\n",
    "categorical_features = train_set.select_dtypes(include=['object']).columns\n",
    "label_encoders = {col: LabelEncoder() for col in categorical_features}\n",
    "\n",
    "for col in categorical_features:\n",
    "    train_set[col] = label_encoders[col].fit_transform(train_set[col])\n",
    "\n",
    "# Feature Selection: Dropping features with very low correlation with the target\n",
    "# We keep some features with moderate correlation and all categorical features\n",
    "features_to_drop = correlation_with_target[abs(correlation_with_target) < 0.05].index\n",
    "train_set_reduced = train_set.drop(features_to_drop, axis=1)\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X = train_set_reduced.drop('monthly_rent', axis=1)\n",
    "y = train_set_reduced['monthly_rent']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the validation set\n",
    "y_pred = rf_regressor.predict(X_val)\n",
    "\n",
    "# Evaluating the model\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "mae, mse, r2\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T17:23:08.988985800Z",
     "start_time": "2023-10-31T17:22:07.646448600Z"
    }
   },
   "id": "7febcad25273da60"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Training a Random Forest Regressor with Polynomial Features\u001B[39;00m\n\u001B[0;32m     11\u001B[0m rf_regressor_poly \u001B[38;5;241m=\u001B[39m RandomForestRegressor(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m rf_regressor_poly\u001B[38;5;241m.\u001B[39mfit(X_train_poly, y_train_poly)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Predicting on the validation set with Polynomial Features\u001B[39;00m\n\u001B[0;32m     15\u001B[0m y_pred_poly \u001B[38;5;241m=\u001B[39m rf_regressor_poly\u001B[38;5;241m.\u001B[39mpredict(X_val_poly)\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    445\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m    447\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    448\u001B[0m ]\n\u001B[0;32m    450\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    451\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[1;32m--> 456\u001B[0m trees \u001B[38;5;241m=\u001B[39m Parallel(\n\u001B[0;32m    457\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs,\n\u001B[0;32m    458\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    459\u001B[0m     prefer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthreads\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    460\u001B[0m )(\n\u001B[0;32m    461\u001B[0m     delayed(_parallel_build_trees)(\n\u001B[0;32m    462\u001B[0m         t,\n\u001B[0;32m    463\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbootstrap,\n\u001B[0;32m    464\u001B[0m         X,\n\u001B[0;32m    465\u001B[0m         y,\n\u001B[0;32m    466\u001B[0m         sample_weight,\n\u001B[0;32m    467\u001B[0m         i,\n\u001B[0;32m    468\u001B[0m         \u001B[38;5;28mlen\u001B[39m(trees),\n\u001B[0;32m    469\u001B[0m         verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    470\u001B[0m         class_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_weight,\n\u001B[0;32m    471\u001B[0m         n_samples_bootstrap\u001B[38;5;241m=\u001B[39mn_samples_bootstrap,\n\u001B[0;32m    472\u001B[0m     )\n\u001B[0;32m    473\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trees)\n\u001B[0;32m    474\u001B[0m )\n\u001B[0;32m    476\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     64\u001B[0m )\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\joblib\\parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\joblib\\parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    186\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[1;32m--> 188\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39mcurr_sample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m     tree\u001B[38;5;241m.\u001B[39mfit(X, y, sample_weight\u001B[38;5;241m=\u001B[39msample_weight, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1149\u001B[0m     )\n\u001B[0;32m   1150\u001B[0m ):\n\u001B[1;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[1;34m(self, X, y, sample_weight, check_input)\u001B[0m\n\u001B[0;32m   1290\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   1291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, check_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1292\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[0;32m   1293\u001B[0m \n\u001B[0;32m   1294\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1317\u001B[0m \u001B[38;5;124;03m        Fitted estimator.\u001B[39;00m\n\u001B[0;32m   1318\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1320\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m_fit(\n\u001B[0;32m   1321\u001B[0m         X,\n\u001B[0;32m   1322\u001B[0m         y,\n\u001B[0;32m   1323\u001B[0m         sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[0;32m   1324\u001B[0m         check_input\u001B[38;5;241m=\u001B[39mcheck_input,\n\u001B[0;32m   1325\u001B[0m     )\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\cs5228-a1\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    433\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[0;32m    434\u001B[0m         splitter,\n\u001B[0;32m    435\u001B[0m         min_samples_split,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    440\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[0;32m    441\u001B[0m     )\n\u001B[1;32m--> 443\u001B[0m builder\u001B[38;5;241m.\u001B[39mbuild(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    446\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Feature Engineering: Experimenting with Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Splitting the polynomial features into training and validation sets\n",
    "X_train_poly, X_val_poly, y_train_poly, y_val_poly = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a Random Forest Regressor with Polynomial Features\n",
    "rf_regressor_poly = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor_poly.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "# Predicting on the validation set with Polynomial Features\n",
    "y_pred_poly = rf_regressor_poly.predict(X_val_poly)\n",
    "\n",
    "# Evaluating the model with Polynomial Features\n",
    "mae_poly = mean_absolute_error(y_val_poly, y_pred_poly)\n",
    "mse_poly = mean_squared_error(y_val_poly, y_pred_poly)\n",
    "r2_poly = r2_score(y_val_poly, y_pred_poly)\n",
    "\n",
    "mae_poly, mse_poly, r2_poly\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T17:39:16.707256700Z",
     "start_time": "2023-10-31T17:26:08.770827900Z"
    }
   },
   "id": "74a51572248f9bcd"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "((48000, 69), (12000, 69))"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Load your datasets\n",
    "train_set = pd.read_csv('./Datasets/final-data/train_set.csv')\n",
    "test_set = pd.read_csv('./Datasets/final-data/test_set.csv')\n",
    "\n",
    "# Assume 'X' contains the features and 'y' is the target variable in your datasets\n",
    "X_train = train_set.drop('monthly_rent', axis=1)\n",
    "y_train = train_set['monthly_rent']\n",
    "X_test = test_set.drop('monthly_rent', axis=1)\n",
    "y_test = test_set['monthly_rent']\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['town', 'region', 'mrt_nearest', 'mall_nearest', 'mrt_planned_nearest']\n",
    "X_train = X_train.drop(columns_to_drop, axis=1)\n",
    "X_test = X_test.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Identify the remaining categorical columns\n",
    "remaining_categorical_columns = ['flat_model', 'subzone', 'planning_area']\n",
    "\n",
    "# Apply one-hot encoding to 'flat_model' and 'planning_area'\n",
    "one_hot_columns = ['flat_model', 'planning_area']\n",
    "\n",
    "# Reinitialize the encoders\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply one-hot encoding to 'flat_model' and 'planning_area'\n",
    "X_train_one_hot = encoder.fit_transform(X_train[one_hot_columns])\n",
    "X_test_one_hot = encoder.transform(X_test[one_hot_columns])\n",
    "X_train_one_hot = pd.DataFrame(X_train_one_hot, columns=encoder.get_feature_names_out(one_hot_columns))\n",
    "X_test_one_hot = pd.DataFrame(X_test_one_hot, columns=encoder.get_feature_names_out(one_hot_columns))\n",
    "\n",
    "# Apply label encoding to 'subzone'\n",
    "X_train['subzone'] = label_encoder.fit_transform(X_train['subzone'])\n",
    "X_test['subzone'] = label_encoder.transform(X_test['subzone'])\n",
    "\n",
    "# Combine the processed data\n",
    "X_train = X_train.drop(one_hot_columns, axis=1)\n",
    "X_test = X_test.drop(one_hot_columns, axis=1)\n",
    "X_train = pd.concat([X_train, X_train_one_hot], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_one_hot], axis=1)\n",
    "\n",
    "# Align the training and test sets\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T18:40:59.531931800Z",
     "start_time": "2023-10-31T18:40:53.482987100Z"
    }
   },
   "id": "c07b6226f2437618"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71a2e1f8275c4ffc8e03efe0dd5fb7ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -240012.45349061466\n",
      "\n",
      "Generation 2 - Current best internal CV score: -240012.45349061466\n",
      "\n",
      "Generation 3 - Current best internal CV score: -239978.23408832456\n",
      "\n",
      "Generation 4 - Current best internal CV score: -238751.41967053938\n",
      "\n",
      "Generation 5 - Current best internal CV score: -238253.8070183262\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(RidgeCV(input_matrix), bootstrap=False, max_features=0.4, min_samples_leaf=17, min_samples_split=10, n_estimators=100)\n",
      "Test Score:  -236929.47970789435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RidgeCV was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=5,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=20,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(\"Test Score: \", tpot.score(X_test, y_test))\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline.py')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T19:17:29.744262400Z",
     "start_time": "2023-10-31T18:41:23.199212900Z"
    }
   },
   "id": "fd6da0ae70078388"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/180 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b32c54d292ca43199d6a21124bef0e73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -240012.45349061466\n",
      "\n",
      "Generation 2 - Current best internal CV score: -240012.45349061466\n",
      "\n",
      "Generation 3 - Current best internal CV score: -239736.105788069\n",
      "\n",
      "Generation 4 - Current best internal CV score: -238388.2665978133\n",
      "\n",
      "Generation 5 - Current best internal CV score: -238007.00834292313\n",
      "\n",
      "Best pipeline: RandomForestRegressor(ElasticNetCV(StandardScaler(input_matrix), l1_ratio=0.75, tol=0.01), bootstrap=True, max_features=0.5, min_samples_leaf=17, min_samples_split=14, n_estimators=100)\n",
      "Test RMSE:  487.8109410530799\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=5,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=30,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error for optimization\n",
    "    warm_start=True  # Reuse the best model from the previous run\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set using RMSE\n",
    "neg_mse = tpot.score(X_test, y_test)\n",
    "rmse = sqrt(-neg_mse)\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T20:21:02.581620200Z",
     "start_time": "2023-10-31T19:24:41.940149800Z"
    }
   },
   "id": "fbddd3b41e748970"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/220 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "199c888e07f245a3b6aeca8750c54cab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -241887.7362343186\n",
      "\n",
      "Generation 2 - Current best internal CV score: -241887.7362343186\n",
      "\n",
      "Generation 3 - Current best internal CV score: -240823.10553396968\n",
      "\n",
      "Generation 4 - Current best internal CV score: -240365.91596491076\n",
      "\n",
      "Generation 5 - Current best internal CV score: -240365.91596491076\n",
      "\n",
      "Generation 6 - Current best internal CV score: -240107.4151266945\n",
      "\n",
      "Generation 7 - Current best internal CV score: -240107.4151266945\n",
      "\n",
      "Generation 8 - Current best internal CV score: -240023.23758409685\n",
      "\n",
      "Generation 9 - Current best internal CV score: -240023.23758409685\n",
      "\n",
      "Generation 10 - Current best internal CV score: -237891.94109066128\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(RidgeCV(input_matrix), bootstrap=True, max_features=1.0, min_samples_leaf=20, min_samples_split=20, n_estimators=100)\n",
      "Test RMSE:  487.0264432087495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but RidgeCV was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=10,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=20,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=21,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error for optimization\n",
    "    warm_start=True  # Reuse the best model from the previous run\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set using RMSE\n",
    "neg_mse = tpot.score(X_test, y_test)\n",
    "rmse = sqrt(-neg_mse)\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T01:29:46.237159800Z",
     "start_time": "2023-11-01T00:37:48.251067300Z"
    }
   },
   "id": "5196f2a8e6d4d456"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/630 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab61d6d117bf4c57a7f629674f7e83e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -240012.45349061466\n",
      "\n",
      "Generation 2 - Current best internal CV score: -240012.45349061466\n",
      "\n",
      "Generation 3 - Current best internal CV score: -240012.45349061466\n",
      "\n",
      "Generation 4 - Current best internal CV score: -239909.02814533917\n",
      "\n",
      "Generation 5 - Current best internal CV score: -238253.8070183262\n",
      "\n",
      "Generation 6 - Current best internal CV score: -238253.8070183262\n",
      "\n",
      "Generation 7 - Current best internal CV score: -238253.8070183262\n",
      "\n",
      "Generation 8 - Current best internal CV score: -238197.862959142\n",
      "\n",
      "Generation 9 - Current best internal CV score: -238197.862959142\n",
      "\n",
      "Generation 10 - Current best internal CV score: -238197.862959142\n",
      "\n",
      "Generation 11 - Current best internal CV score: -238098.93263668864\n",
      "\n",
      "Generation 12 - Current best internal CV score: -237737.3505477355\n",
      "\n",
      "Generation 13 - Current best internal CV score: -237728.15459490352\n",
      "\n",
      "Generation 14 - Current best internal CV score: -237728.15459490352\n",
      "\n",
      "Generation 15 - Current best internal CV score: -237728.15459490352\n",
      "\n",
      "Generation 16 - Current best internal CV score: -237728.15459490352\n",
      "\n",
      "Generation 17 - Current best internal CV score: -237728.15459490352\n",
      "\n",
      "Generation 18 - Current best internal CV score: -237728.15459490352\n",
      "\n",
      "Generation 19 - Current best internal CV score: -237728.15459490352\n",
      "\n",
      "Generation 20 - Current best internal CV score: -235350.64327944428\n",
      "\n",
      "Best pipeline: XGBRegressor(input_matrix, learning_rate=0.1, max_depth=6, min_child_weight=3, n_estimators=100, n_jobs=1, objective=reg:squarederror, subsample=1.0, verbosity=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  484.13895153749746\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=20,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=30,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error for optimization\n",
    "    warm_start=True  # Reuse the best model from the previous run\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set using RMSE\n",
    "neg_mse = tpot.score(X_test, y_test)\n",
    "rmse = sqrt(-neg_mse)\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T04:28:29.819330400Z",
     "start_time": "2023-11-01T01:38:57.387837200Z"
    }
   },
   "id": "8303a6c88e0f6a15"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/60 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba9b3f31123a4e2db075e0a56683fa73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -234589.1031424426\n",
      "\n",
      "Generation 2 - Current best internal CV score: -234589.1031424426\n",
      "\n",
      "Generation 3 - Current best internal CV score: -234589.1031424426\n",
      "\n",
      "Generation 4 - Current best internal CV score: -234584.95256375504\n",
      "\n",
      "Generation 5 - Current best internal CV score: -234584.95256375504\n",
      "\n",
      "Best pipeline: XGBRegressor(input_matrix, learning_rate=0.02, max_depth=5, min_child_weight=5, n_estimators=500, n_jobs=1, subsample=1.0)\n",
      "Test RMSE:  483.54129762004953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "config_dict = {\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'n_estimators': [5, 50, 100, 250, 300, 500],\n",
    "        'max_depth': [1, 3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 10],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [1, 2, 3, 4, 5],\n",
    "        'n_jobs': [1]  # TPOT is already parallelized, so set n_jobs to 1 for XGBoost\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=5,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=10,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error for optimization\n",
    "    config_dict=config_dict,  # Custom configuration\n",
    "    warm_start=True  # Reuse the best model from the previous run\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set using RMSE\n",
    "neg_mse = tpot.score(X_test, y_test)\n",
    "rmse = sqrt(-neg_mse)\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T05:17:39.214756Z",
     "start_time": "2023-11-01T04:53:26.352026800Z"
    }
   },
   "id": "b3902d7d8c28dc99"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/220 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c32cf7c83995434e98b7a3b42dc121db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -233827.96483994595\n",
      "\n",
      "Generation 2 - Current best internal CV score: -233827.96483994595\n",
      "\n",
      "Generation 3 - Current best internal CV score: -233827.96483994595\n",
      "\n",
      "Generation 4 - Current best internal CV score: -233688.91533545233\n",
      "\n",
      "Generation 5 - Current best internal CV score: -233688.91533545233\n",
      "\n",
      "Generation 6 - Current best internal CV score: -233688.91533545233\n",
      "\n",
      "Generation 7 - Current best internal CV score: -233688.91533545233\n",
      "\n",
      "Generation 8 - Current best internal CV score: -233688.91533545233\n",
      "\n",
      "Generation 9 - Current best internal CV score: -233688.91533545233\n",
      "\n",
      "Generation 10 - Current best internal CV score: -233688.91533545233\n",
      "\n",
      "Best pipeline: XGBRegressor(input_matrix, learning_rate=0.025, max_depth=5, min_child_weight=8, n_estimators=600, n_jobs=1, subsample=0.8)\n",
      "Test RMSE:  482.99408559647253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "config_dict = {\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'n_estimators': [400, 500, 600, 700, 1000],\n",
    "        'max_depth': [4, 5, 6, 9],\n",
    "        'learning_rate': [0.01, 0.015, 0.02, 0.025, 0.05, 0.1, 0.5, 1],\n",
    "        'subsample': [0.8, 0.9, 1.0, 1.1, 1.2, 1.3,1.5],\n",
    "        'min_child_weight': [4, 5, 6, 7, 8],\n",
    "        'n_jobs': [1]  # TPOT is already parallelized, so set n_jobs to 1 for XGBoost\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=10,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=20,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error for optimization\n",
    "    config_dict=config_dict,  # Custom configuration\n",
    "    warm_start=True  # Reuse the best model from the previous run\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set using RMSE\n",
    "neg_mse = tpot.score(X_test, y_test)\n",
    "rmse = sqrt(-neg_mse)\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:15:35.143622600Z",
     "start_time": "2023-11-01T05:26:52.957719200Z"
    }
   },
   "id": "1391893a9f60de4d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './Datasets/final-data/enhanced_train_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 32\u001B[0m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m# Saving the enhanced dataset to a CSV file\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     enhanced_dataset_csv_path \u001B[38;5;241m=\u001B[39m output_file_path[idx]\n\u001B[1;32m---> 32\u001B[0m     \u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43menhanced_dataset_csv_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m enhanced_dataset_csv_path\n",
      "File \u001B[1;32m~\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3891\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3893\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3894\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3895\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3899\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3900\u001B[0m )\n\u001B[1;32m-> 3902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3903\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3904\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3905\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3906\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3907\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3908\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3909\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3910\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3911\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3912\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3913\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3914\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m   1131\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1133\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m   1134\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m   1135\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1150\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1151\u001B[0m )\n\u001B[1;32m-> 1152\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1155\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32m~\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    244\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[0;32m    245\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[1;32m--> 247\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    249\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    250\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    251\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    257\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    258\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    264\u001B[0m     )\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[1;32m~\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    864\u001B[0m             handle,\n\u001B[0;32m    865\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m    866\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[0;32m    867\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m    868\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    869\u001B[0m         )\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: './Datasets/final-data/enhanced_train_set.csv'"
     ]
    }
   ],
   "source": [
    "# Re-import necessary libraries and re-save the file after code execution state reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Re-load the original dataset\n",
    "train_file_path = ['./Datasets/final-data/train_set.csv', './Datasets/final-data/test_set.csv']\n",
    "output_file_path = ['./Datasets/final-data/enhanced_train_set.csv', './Datasets/final-data/enhanced_test_set.csv']\n",
    "for idx, path in enumerate(train_file_path):\n",
    "    train_data = pd.read_csv(train_file_path[idx])\n",
    "    \n",
    "    # Reapply the transformations and feature engineering\n",
    "    # Logarithmic transformation of 'monthly_rent'\n",
    "    train_data['log_monthly_rent'] = np.log(train_data['monthly_rent'])\n",
    "    \n",
    "    # Temporal features\n",
    "    train_data['lease_age_at_rent_approval'] = train_data['rent_approval_year'] - train_data['lease_commence_date']\n",
    "\n",
    "    # Pad the 'Month' column with leading zeros to ensure two-digit format\n",
    "    train_data['Month'] = train_data['Month'].apply(lambda x: f'{x:02d}')\n",
    "    \n",
    "    # Combine 'Year' and 'Month' into a single string and then convert to integer\n",
    "    train_data['year_month'] = (train_data['Year'].astype(str) + train_data['Month']).astype(int)\n",
    "    \n",
    "    # Geographical clustering\n",
    "    from sklearn.cluster import KMeans\n",
    "    geo_features = train_data[['latitude', 'longitude']]\n",
    "    kmeans = KMeans(n_clusters=5, random_state=0).fit(geo_features)\n",
    "    train_data['geo_cluster'] = kmeans.labels_\n",
    "    \n",
    "    # Saving the enhanced dataset to a CSV file\n",
    "    enhanced_dataset_csv_path = output_file_path[idx]\n",
    "    train_data.to_csv(enhanced_dataset_csv_path, index=False)\n",
    "\n",
    "enhanced_dataset_csv_path\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T10:03:09.544881500Z",
     "start_time": "2023-11-01T10:03:08.945222900Z"
    }
   },
   "id": "94e3e26686814"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "((48000, 70), (12000, 70))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Load your datasets\n",
    "train_set = pd.read_csv('./Datasets/final-data/enhanced_train_set.csv')\n",
    "test_set = pd.read_csv('./Datasets/final-data/enhanced_test_set.csv')\n",
    "\n",
    "# Assume 'X' contains the features and 'y' is the target variable in your datasets\n",
    "X_train = train_set.drop('log_monthly_rent', axis=1)\n",
    "y_train = train_set['log_monthly_rent']\n",
    "X_test = test_set.drop('log_monthly_rent', axis=1)\n",
    "y_test = test_set['log_monthly_rent']\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = ['town', 'region', 'mrt_nearest', 'mall_nearest', 'mrt_planned_nearest', 'Year', 'Month', 'monthly_rent']\n",
    "X_train = X_train.drop(columns_to_drop, axis=1)\n",
    "X_test = X_test.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Identify the remaining categorical columns\n",
    "remaining_categorical_columns = ['flat_model', 'subzone', 'planning_area']\n",
    "\n",
    "# Apply one-hot encoding to 'flat_model' and 'planning_area'\n",
    "one_hot_columns = ['flat_model', 'planning_area']\n",
    "\n",
    "# Reinitialize the encoders\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply one-hot encoding to 'flat_model' and 'planning_area'\n",
    "X_train_one_hot = encoder.fit_transform(X_train[one_hot_columns])\n",
    "X_test_one_hot = encoder.transform(X_test[one_hot_columns])\n",
    "X_train_one_hot = pd.DataFrame(X_train_one_hot, columns=encoder.get_feature_names_out(one_hot_columns))\n",
    "X_test_one_hot = pd.DataFrame(X_test_one_hot, columns=encoder.get_feature_names_out(one_hot_columns))\n",
    "\n",
    "# Apply label encoding to 'subzone'\n",
    "X_train['subzone'] = label_encoder.fit_transform(X_train['subzone'])\n",
    "X_test['subzone'] = label_encoder.transform(X_test['subzone'])\n",
    "\n",
    "# Combine the processed data\n",
    "X_train = X_train.drop(one_hot_columns, axis=1)\n",
    "X_test = X_test.drop(one_hot_columns, axis=1)\n",
    "X_train = pd.concat([X_train, X_train_one_hot], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_one_hot], axis=1)\n",
    "\n",
    "# Align the training and test sets\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T10:03:12.558141900Z",
     "start_time": "2023-11-01T10:03:12.304156Z"
    }
   },
   "id": "87e68db4e5f64995"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/220 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd8c2caed7454853866a7186a953ed00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -0.035066762105337296\n",
      "\n",
      "Generation 2 - Current best internal CV score: -0.035066762105337296\n",
      "\n",
      "Generation 3 - Current best internal CV score: -0.035066762105337296\n",
      "\n",
      "Generation 4 - Current best internal CV score: -0.035066762105337296\n",
      "\n",
      "Generation 5 - Current best internal CV score: -0.03504479889238586\n",
      "\n",
      "Generation 6 - Current best internal CV score: -0.03501728804707829\n",
      "\n",
      "Generation 7 - Current best internal CV score: -0.03501728804707829\n",
      "\n",
      "Generation 8 - Current best internal CV score: -0.03501728804707829\n",
      "\n",
      "Generation 9 - Current best internal CV score: -0.03501728804707829\n",
      "\n",
      "Generation 10 - Current best internal CV score: -0.03501728804707829\n",
      "\n",
      "Best pipeline: XGBRegressor(input_matrix, learning_rate=0.05, max_depth=4, min_child_weight=4, n_estimators=500, n_jobs=1, subsample=0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 40\u001B[0m\n\u001B[0;32m     36\u001B[0m predictions \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(predictions_log_scale)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Ensure y_test contains the original monthly_rent values (not log transformed)\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# Calculate RMSE on the original scale of monthly rent\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m rmse \u001B[38;5;241m=\u001B[39m sqrt(\u001B[43mmean_squared_error\u001B[49m(y_test, predictions))\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest RMSE: \u001B[39m\u001B[38;5;124m\"\u001B[39m, rmse)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# Export the best pipeline as a Python script\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "config_dict = {\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'n_estimators': [400, 500, 600, 700, 1000],\n",
    "        'max_depth': [4, 5, 6, 9],\n",
    "        'learning_rate': [0.01, 0.015, 0.02, 0.025, 0.05, 0.1, 0.5, 1],\n",
    "        'subsample': [0.8, 0.9, 1.0, 1.1, 1.2, 1.3,1.5],\n",
    "        'min_child_weight': [4, 5, 6, 7, 8],\n",
    "        'n_jobs': [1],  # TPOT is already parallelized, so set n_jobs to 1 for XGBoost\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=10,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=20,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error for optimization\n",
    "    config_dict=config_dict,  # Custom configuration\n",
    "    warm_start=True  # Reuse the best model from the previous run\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T11:51:37.309609500Z",
     "start_time": "2023-11-01T10:03:20.592739900Z"
    }
   },
   "id": "fb4ab729e5c3423"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('xgbregressor',\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None, learning_rate=0.05,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=4, max_leaves=None, min_child_weight=4,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=500, n_jobs=1,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=42, ...))])",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;xgbregressor&#x27;,\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None, learning_rate=0.05,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=4, max_leaves=None, min_child_weight=4,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=500, n_jobs=1,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=42, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;xgbregressor&#x27;,\n                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, early_stopping_rounds=None,\n                              enable_categorical=False, eval_metric=None,\n                              feature_types=None, gamma=None, gpu_id=None,\n                              grow_policy=None, importance_type=None,\n                              interaction_constraints=None, learning_rate=0.05,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=4, max_leaves=None, min_child_weight=4,\n                              missing=nan, monotone_constraints=None,\n                              n_estimators=500, n_jobs=1,\n                              num_parallel_tree=None, predictor=None,\n                              random_state=42, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=4, missing=nan, monotone_constraints=None,\n             n_estimators=500, n_jobs=1, num_parallel_tree=None, predictor=None,\n             random_state=42, ...)</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = tpot.fitted_pipeline_\n",
    "best_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T11:59:06.318596600Z",
     "start_time": "2023-11-01T11:59:06.294496300Z"
    }
   },
   "id": "477c482de6bf0c36"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  2585.2487913373884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "C:\\Users\\User\\.conda\\envs\\Project-tpot\\Lib\\site-packages\\xgboost\\data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt, exp\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Extract the best fitted pipeline from TPOT\n",
    "best_model = tpot.fitted_pipeline_\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_log_scale = best_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to the original scale\n",
    "predictions = np.exp(predictions_log_scale)\n",
    "\n",
    "# Ensure y_test is on the original scale (monthly_rent, not log_monthly_rent)\n",
    "# Calculate RMSE on the original scale of monthly rent\n",
    "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline_1.py')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T11:53:37.147284300Z",
     "start_time": "2023-11-01T11:53:37.014106100Z"
    }
   },
   "id": "300ba0f960690fba"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/220 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9acad2a68f0b4b79b78c1490bbbbcf82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -0.03595816267649651\n",
      "\n",
      "Generation 2 - Current best internal CV score: -0.03595816267649651\n",
      "\n",
      "Generation 3 - Current best internal CV score: -0.03580272778775873\n",
      "\n",
      "Generation 4 - Current best internal CV score: -0.03576255521206333\n",
      "\n",
      "Generation 5 - Current best internal CV score: -0.03576255521206333\n",
      "\n",
      "Generation 6 - Current best internal CV score: -0.03576255521206333\n",
      "\n",
      "Generation 7 - Current best internal CV score: -0.03571233439338654\n",
      "\n",
      "Generation 8 - Current best internal CV score: -0.03571233439338654\n",
      "\n",
      "Generation 9 - Current best internal CV score: -0.03567305049940886\n",
      "\n",
      "Generation 10 - Current best internal CV score: -0.03567305049940886\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(ElasticNetCV(RobustScaler(input_matrix), l1_ratio=0.75, tol=0.01), bootstrap=True, max_features=0.7000000000000001, min_samples_leaf=18, min_samples_split=6, n_estimators=100)\n",
      "Test RMSE:  2581.64943442527\n"
     ]
    }
   ],
   "source": [
    "# Create a TPOT regressor\n",
    "tpot = TPOTRegressor(\n",
    "    generations=10,  # Number of iterations to the run pipeline optimization process\n",
    "    population_size=20,  # Number of individuals to retain in the genetic programming population every generation\n",
    "    verbosity=2,  # Show progress\n",
    "    random_state=42,  # Seed for reproducibility\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='neg_mean_squared_error',  # Use negative mean squared error for optimization\n",
    "    warm_start=True  # Reuse the best model from the previous run\n",
    ")\n",
    "\n",
    "# Run the TPOT optimization\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best fitted pipeline from TPOT\n",
    "best_model = tpot.fitted_pipeline_\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_log_scale = best_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back to the original scale\n",
    "predictions = np.exp(predictions_log_scale)\n",
    "\n",
    "# Ensure y_test is on the original scale (monthly_rent, not log_monthly_rent)\n",
    "# Calculate RMSE on the original scale of monthly rent\n",
    "y_test_original_scale = np.exp(y_test)\n",
    "rmse = sqrt(mean_squared_error(y_test_original_scale, predictions))\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline_1.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:29:59.873162500Z",
     "start_time": "2023-11-01T12:31:15.165097600Z"
    }
   },
   "id": "280cbbbd5ce68c79"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:  489.892912764877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure y_test is on the original scale (monthly_rent, not log_monthly_rent)\n",
    "# Calculate RMSE on the original scale of monthly rent\n",
    "y_test_original_scale = np.exp(y_test)\n",
    "rmse = sqrt(mean_squared_error(y_test_original_scale, predictions))\n",
    "print(\"Test RMSE: \", rmse)\n",
    "\n",
    "# Export the best pipeline as a Python script\n",
    "tpot.export('best_pipeline_1.py')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T13:31:48.628680700Z",
     "start_time": "2023-11-01T13:31:48.396676900Z"
    }
   },
   "id": "9d3b70a1fb368991"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
